{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Laporan Praktikum Web Scraping**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Denis Muhammad Jethro/ 162112133028\n",
    "\n",
    "Github : https://github.com/Juethro/pythonWeb-scraper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests as rq\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import pandas as pd\n",
    "import re\n",
    "import scrapy as sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Scraping Beautifulsoup**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step:\n",
    "\n",
    "1. Ambil html dengan requests\n",
    "2. Masukkan dalam file .html\n",
    "3. Buka file dan masukkan dalam Beautifulsoup \n",
    "4. Find_all tag yang dicari, dalam hal ini *h3*\n",
    "5. Bersihkan hasilnya dan buat dataframe dari kumpulan data tersebut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = rq.get(\"https://unair.ac.id/news\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save in html format and Open it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"index.html\", 'w', encoding='utf_8') as nulis:\n",
    "    nulis.write(url.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"index.html\", 'r', encoding='utf_8') as htmll:\n",
    "    soup = bs(htmll, 'html.parser')\n",
    "\n",
    "listt_title = soup.find_all('h3' , class_='elementor-post__title')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning Data *listt_title*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_judul = []\n",
    "filter = []\n",
    "\n",
    "for i in listt_title: \n",
    "    splitted = re.split(r'[\\n\\t\\f\\v\\r]+', i.find('a').get_text())\n",
    "    \n",
    "    # Hilangkan enter depan\n",
    "    for x in splitted[1:]:\n",
    "        filter.append(x)\n",
    "\n",
    "# Hilangkan enter belakang\n",
    "for su in filter[0::2]:\n",
    "    data_judul.append(su)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make DataFrame and Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Judul_Headline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Kontribusi Terhadap Keberlanjutan Lingkungan, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jalani Hidup Sehat, Ini Yang Perlu Disiapkan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5 Mahasiswa UNAIR Ikuti Student Exchange dan K...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Waspadai Resistensi Antibiotik Saat Sakit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Kontribusi Terhadap Keberlanjutan Lingkungan, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      Judul_Headline\n",
       "0  Kontribusi Terhadap Keberlanjutan Lingkungan, ...\n",
       "1      Jalani Hidup Sehat, Ini Yang Perlu DisiapkanÂ \n",
       "2  5 Mahasiswa UNAIR Ikuti Student Exchange dan K...\n",
       "3          Waspadai Resistensi Antibiotik Saat Sakit\n",
       "4  Kontribusi Terhadap Keberlanjutan Lingkungan, ..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(data_judul)\n",
    "df.columns = ['Judul_Headline']\n",
    "df.to_csv('../scraping-headlines.csv')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Terlihat file csv sudah terekspor dan isinya sesuai dengan apa yang ada di website *unair news* tersebut "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Crawling Beautifulsoup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = rq.get('https://unair.ac.id/news')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Melakukan http requests ke server dan disimpan dalam variabel `urls`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save html File and Open it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('index2.html', 'w', encoding= 'utf_8') as f:\n",
    "    f.write(urls.text)\n",
    "\n",
    "with open('index2.html', 'r') as duar:\n",
    "    soup = bs(urls.text, 'html.parser')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Untuk menghemat tempat di ipynb, saya ekspor file html tersebut dengan nama `index2.html`. Jika ingin melihat isinya bisa dilihat di sana."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make crawler route"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dapatkan semua link artikel\n",
    "all_links = soup.find_all('a', class_='elementor-post__read-more')\n",
    "news_links = []\n",
    "\n",
    "for i in all_links: \n",
    "    ab = i['href']\n",
    "    news_links.append(ab)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dari file html tersebut, saya cari semua tag a dengan class `elementor-post__read-more` kemudian melakukan iterasi untuk memfilter `href` dan dimasukkan dalam variabel `news_links`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crawl from news_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_judul = []\n",
    "list_date = []\n",
    "#crawler unit (masih bisa dikembangkan)\n",
    "for i in news_links:\n",
    "    buang = rq.get(i.lstrip(\"\\'\"))\n",
    "\n",
    "    bakar = bs(buang.text, 'html.parser')\n",
    "    judul = bakar.find_all('h3', class_='elementor-heading-title')\n",
    "    date = bakar.find_all('span', class_='elementor-post-info__item--type-date')\n",
    "\n",
    "    list_judul.append(judul[0].string)\n",
    "    list_date.append(date[0].string)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Melakukan crawl dengan data link dari `news_links` kemudian di tiap link mengambil tag `h3` dengan class `elementor-heading-title` dan tag `span` dengan class `elementor-post-info__item--type-date` yakni judul artikel dan tanggal uploadnya. Kedua data tersebut dimasukkan ke dalam dua list yakni `list-judul` dan `list-date`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning Data *Date*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_date = []\n",
    "filtered_date = []\n",
    "\n",
    "for so in list_date: \n",
    "    splitted = re.split(r'[\\n\\t\\f\\v\\r]+', so)\n",
    "    \n",
    "    # Hilangkan enter depan\n",
    "    for x in splitted[1:]:\n",
    "        raw_date.append(x)\n",
    "\n",
    "# Hilangkan enter belakang\n",
    "for su in raw_date[0::2]:\n",
    "    filtered_date.append(su)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kode diatas digunakan untuk membersihkan data *date* yang tidak bersih dikarenakan banyak tulisan \\n \\t dsb. Gangguan tersebut berada pada depan judul dan diakhir masing-masing judul, sehingga mulai melakukan penghapusan *noise* tersebut"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make and Export DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Judul</th>\n",
       "      <th>Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Kontribusi Terhadap Keberlanjutan Lingkungan, ...</td>\n",
       "      <td>Oktober 25, 2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jalani Hidup Sehat, Ini Yang Perlu Disiapkan</td>\n",
       "      <td>Oktober 26, 2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5 Mahasiswa UNAIR Ikuti Student Exchange dan K...</td>\n",
       "      <td>Oktober 25, 2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Waspadai Resistensi Antibiotik Saat Sakit</td>\n",
       "      <td>Oktober 25, 2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Kontribusi Terhadap Keberlanjutan Lingkungan, ...</td>\n",
       "      <td>Oktober 25, 2022</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Judul              Date\n",
       "0  Kontribusi Terhadap Keberlanjutan Lingkungan, ...  Oktober 25, 2022\n",
       "1      Jalani Hidup Sehat, Ini Yang Perlu DisiapkanÂ   Oktober 26, 2022\n",
       "2  5 Mahasiswa UNAIR Ikuti Student Exchange dan K...  Oktober 25, 2022\n",
       "3          Waspadai Resistensi Antibiotik Saat Sakit  Oktober 25, 2022\n",
       "4  Kontribusi Terhadap Keberlanjutan Lingkungan, ...  Oktober 25, 2022"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = pd.DataFrame({'Judul' : list_judul, 'Date' : filtered_date})\n",
    "df2.to_csv('../crawling-headlines.csv')\n",
    "\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Melakukan ekspor dataframe dalam format csv dan terlihat, dataset sudah sesuai dengan yang diinginkan sehingga bisa dilakukan analisis lebih lanjut"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Scrapy Crawler**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup VirtualENV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![venv](documentation/setup-venv.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![venv](documentation/setup-venv2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Membuat venv atau virtual environment, dikarenakan rumor yang beredar tentang scrapy suka merusak evironment python. Untuk mengatasinya saya membuat venv yang nanti akan menjalankan crawling scrapy selanjutnya."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Scrapy Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![venv](documentation/venv-active.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![scrapy](documentation/scrapy-startproject.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step:\n",
    "1. Aktifkan venv `(gambar-1)`\n",
    "2. Start project `(gambar-2)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Membuat projek scrapy, ini dilakukan agar scrapy membuat folder berisi komponen spider. Komponen ini berfungsi agar crawling dapat dilakukan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spider code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Spider code\n",
    "import scrapy\n",
    "\n",
    "class Ayomain_Spider(scrapy.Spider):\n",
    "    name = 'ayomain' #nama spider\n",
    "    start_urls = [\"https://store.playstation.com/en-id/category/29696e1b-a942-4832-935d-ebd11b163263/\"] #url awal\n",
    "    \n",
    "\n",
    "    def parse(self, response):\n",
    "        url = response.url\n",
    "        \n",
    "        for i in range(1,47): #iterasi sampe page ke-46\n",
    "            yield scrapy.Request(url=url+str(i), callback=self.parse_details) #url awal ditambah page ke-sekian\n",
    "            \n",
    "    def parse_details(self, response):\n",
    "        for text in response.css(\".psw-product-tile__details\"): \n",
    "            yield{\n",
    "                \"title\":text.css(\".psw-t-body::text\").get(), #filter yang memiliki class .psw-t-body\n",
    "                \"price\":text.css(\".psw-m-r-3::text\").get()}  #filter yang memiliki class .psw-m-r-3\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Membuat sebuah file python baru dengan nama file `play_spider.py` di dalam direktori spider. Di dalam direktori tersebut berisi spider yang akan melakukan crawl."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start Spider Crawl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![crawl](documentation/start-crawling.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Memulai crawling dengan perintah diatas `scrapy crawl ayomain -t json -o titleNprice.json`.\n",
    "\n",
    "`scrapy` : memanggil modul scrapy\n",
    "\n",
    "`crawl`  : command untuk memulai crawl\n",
    "\n",
    "`ayomain`: nama spider yang sudah dibuat tadi\n",
    "\n",
    "`json`   : mengeluarkan output file json berisi data yang diambil berdasarkan codingan pada spider. Dalam kasus ini ambil title game dan pricenya"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "39557e63b7570cc9c16dbe2f4352c377e82e370b7e8c52a7d26f90d7cc453b6e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
